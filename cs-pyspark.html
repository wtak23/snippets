

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>5. pyspark &mdash; Snippets 1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon-penn.ico"/>
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Snippets 1 documentation" href="index.html"/>
        <link rel="next" title="6. Brain Connectivity Toolbox" href="bct.html"/>
        <link rel="prev" title="4. Python" href="cs-python.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Snippets
          

          
          </a>

          
            
            
              <div class="version">
                1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="cs-bash-commands.html">1. bash-commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-bash-scripts.html">2. bash-scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-qsubber.html">3. qsubber</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-python.html">4. Python</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">5. pyspark</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#stack-overflows">5.1. Stack overflows</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#numpy-array-to-spark-dataframe-trickier-than-i-thought">5.1.1. numpy array to spark dataframe...trickier than i thought...</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adding-new-columns-to-spark-dataframes">5.1.2. Adding new columns to Spark DataFrames</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#handy-snippets">5.2. Handy snippets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#todo-run-this-script">5.2.1. Todo: run this script</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bunch-of-example-scripts">5.2.2. Bunch of example scripts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#use-of-persist-rdd-persistence">5.2.3. Use of persist (RDD persistence)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#sql-in-pyspark">5.3. SQL in pyspark</a></li>
<li class="toctree-l2"><a class="reference internal" href="#spark-sql-dataframes-and-datasets-guide">5.4. Spark SQL, DataFrames and Datasets Guide</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dataframes">5.4.1. DataFrames</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inter-operating-rdd-dataframes">5.4.2. Inter-operating RDD &amp; DataFrames</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-sources">5.4.3. Data Sources</a></li>
<li class="toctree-l3"><a class="reference internal" href="#saving-to-persistent-tables">5.4.4. Saving to Persistent Tables</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#parquet-files">5.4.4.1. Parquet Files</a></li>
<li class="toctree-l4"><a class="reference internal" href="#schema-merging">5.4.4.2. Schema merging</a></li>
<li class="toctree-l4"><a class="reference internal" href="#json-datasets">5.4.4.3. JSON Datasets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#programming-guide-condensed-summary">5.5. Programming-guide: condensed summary</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#super-basics">5.5.1. Super basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#shared-variables">5.5.2. Shared variables</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#broadast-variables">5.5.2.1. Broadast variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="#accumulators">5.5.2.2. Accumulators</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#rdd-operations">5.5.3. RDD operations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#rdd-basics">5.5.3.1. RDD-basics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#passing-functions-to-spark">5.5.3.2. Passing functions to Spark</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rdd-transformations">5.5.3.3. RDD Transformations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rdd-actions">5.5.3.4. RDD Actions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#closures">5.5.3.5. closures</a></li>
<li class="toctree-l4"><a class="reference internal" href="#working-with-key-value-pairs">5.5.3.6. Working with key-value pairs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#random-handy-snippets">5.6. Random handy snippets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#compute-average">5.6.1. compute average</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bct.html">6. Brain Connectivity Toolbox</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-sphinx.html">7. sphinx</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-R.html">8. R</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-awk-oneliners.html">9. awk-oneliners</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-perl-oneliners.html">10. perl one-liners</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-py-jupyter-notebook.html">11. python-jupyter-notebook</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-regexp.html">12. regular expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-rst.html">13. rst</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-rst-old.html">14. rst cheatsheet (old)</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-sed-oneliners.html">15. sed oneliners</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-sed.html">16. sed</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-sql.html">17. sql</a></li>
<li class="toctree-l1"><a class="reference internal" href="cs-scala.html">18. Scala</a></li>
<li class="toctree-l1"><a class="reference internal" href="awk-tutorial/index.html">19. awk-tutorial</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Snippets</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          













<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>5. pyspark</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/cs-pyspark.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="pyspark">
<h1><a class="toc-backref" href="#id1">5. pyspark</a><a class="headerlink" href="#pyspark" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference external" href="http://spark.apache.org/docs/latest/programming-guide.html">http://spark.apache.org/docs/latest/programming-guide.html</a></li>
<li><a class="reference external" href="http://spark.apache.org/docs/latest/submitting-applications.html">http://spark.apache.org/docs/latest/submitting-applications.html</a></li>
<li><a class="reference external" href="https://wtak23.github.io/pyspark_doc/index.html">https://wtak23.github.io/pyspark_doc/index.html</a></li>
<li><a class="reference external" href="https://databricks.com/resources/type/example-notebooks">https://databricks.com/resources/type/example-notebooks</a></li>
</ul>
<p>Good tuorials</p>
<ul class="simple">
<li><a class="reference external" href="https://s3.amazonaws.com/sparksummit-share/ml-ams-1.0.1/index.html">https://s3.amazonaws.com/sparksummit-share/ml-ams-1.0.1/index.html</a></li>
<li><a class="reference external" href="https://spark-summit.org/2016/">Spark 2016 summit</a></li>
</ul>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first"><strong>Table of Contents</strong></p>
<ul class="simple">
<li><a class="reference internal" href="#pyspark" id="id1">pyspark</a><ul>
<li><a class="reference internal" href="#stack-overflows" id="id2">Stack overflows</a><ul>
<li><a class="reference internal" href="#numpy-array-to-spark-dataframe-trickier-than-i-thought" id="id3">numpy array to spark dataframe...trickier than i thought...</a></li>
<li><a class="reference internal" href="#adding-new-columns-to-spark-dataframes" id="id4">Adding new columns to Spark DataFrames</a></li>
</ul>
</li>
<li><a class="reference internal" href="#handy-snippets" id="id5">Handy snippets</a><ul>
<li><a class="reference internal" href="#todo-run-this-script" id="id6">Todo: run this script</a></li>
<li><a class="reference internal" href="#bunch-of-example-scripts" id="id7">Bunch of example scripts</a></li>
<li><a class="reference internal" href="#use-of-persist-rdd-persistence" id="id8">Use of persist (RDD persistence)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sql-in-pyspark" id="id9">SQL in pyspark</a></li>
<li><a class="reference internal" href="#spark-sql-dataframes-and-datasets-guide" id="id10">Spark SQL, DataFrames and Datasets Guide</a><ul>
<li><a class="reference internal" href="#dataframes" id="id11">DataFrames</a></li>
<li><a class="reference internal" href="#inter-operating-rdd-dataframes" id="id12">Inter-operating RDD &amp; DataFrames</a></li>
<li><a class="reference internal" href="#data-sources" id="id13">Data Sources</a></li>
<li><a class="reference internal" href="#saving-to-persistent-tables" id="id14">Saving to Persistent Tables</a></li>
</ul>
</li>
<li><a class="reference internal" href="#programming-guide-condensed-summary" id="id15">Programming-guide: condensed summary</a><ul>
<li><a class="reference internal" href="#super-basics" id="id16">Super basics</a></li>
<li><a class="reference internal" href="#shared-variables" id="id17">Shared variables</a></li>
<li><a class="reference internal" href="#rdd-operations" id="id18">RDD operations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#random-handy-snippets" id="id19">Random handy snippets</a><ul>
<li><a class="reference internal" href="#compute-average" id="id20">compute average</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="stack-overflows">
<h2><a class="toc-backref" href="#id2">5.1. Stack overflows</a><a class="headerlink" href="#stack-overflows" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><a class="reference external" href="http://stackoverflow.com/questions/30787635/takeordered-descending-pyspark">http://stackoverflow.com/questions/30787635/takeordered-descending-pyspark</a></li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">RDD</span><span class="o">.</span><span class="n">takeOrdered</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># sort by keys</span>
<span class="n">RDD</span><span class="o">.</span><span class="n">takeOrdered</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># sort by keys (descending)</span>
<span class="n">RDD</span><span class="o">.</span><span class="n">takeOrdered</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># sort by values</span>
<span class="n">RDD</span><span class="o">.</span><span class="n">takeOrdered</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">key</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># sort by values (descending)</span>
</pre></div>
</div>
<div class="section" id="numpy-array-to-spark-dataframe-trickier-than-i-thought">
<h3><a class="toc-backref" href="#id3">5.1.1. numpy array to spark dataframe...trickier than i thought...</a><a class="headerlink" href="#numpy-array-to-spark-dataframe-trickier-than-i-thought" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://stackoverflow.com/questions/32742004/create-spark-dataframe-can-not-infer-schema-for-type-type-float">http://stackoverflow.com/questions/32742004/create-spark-dataframe-can-not-infer-schema-for-type-type-float</a></li>
</ul>
<p>This bit me in the ass during edX...</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">myFloatRdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">3.0</span><span class="p">])</span>
<span class="c1">#df = myFloatRdd.toDF() #&lt;- won&#39;t work1 raises a TypeError</span>
<span class="n">myFloatRdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">))</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> <span class="c1"># &lt;- need to give in tuple format...</span>
<span class="c1">#+---+</span>
<span class="c1">#| _1|</span>
<span class="c1">#+---+</span>
<span class="c1">#|1.0|</span>
<span class="c1">#|2.0|</span>
<span class="c1">#|3.0|</span>
<span class="c1">#+---+</span>

<span class="c1"># or even better...</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="n">row</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s2">&quot;val&quot;</span><span class="p">)</span> <span class="c1"># Or some other column name</span>
<span class="n">myFloatRdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">row</span><span class="p">)</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">#+---+</span>
<span class="c1">#|val|</span>
<span class="c1">#+---+</span>
<span class="c1">#|1.0|</span>
<span class="c1">#|2.0|</span>
<span class="c1">#|3.0|</span>
<span class="c1">#+---+</span>
</pre></div>
</div>
</div>
<div class="section" id="adding-new-columns-to-spark-dataframes">
<h3><a class="toc-backref" href="#id4">5.1.2. Adding new columns to Spark DataFrames</a><a class="headerlink" href="#adding-new-columns-to-spark-dataframes" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="http://stackoverflow.com/questions/33681487/how-do-i-add-a-new-column-to-spark-data-frame-pyspark">http://stackoverflow.com/questions/33681487/how-do-i-add-a-new-column-to-spark-data-frame-pyspark</a></li>
</ul>
<p>You cannot add an arbitrary column to a <code class="docutils literal"><span class="pre">DataFrame</span></code> in Spark.</p>
<p>4 approaches:</p>
<ol class="arabic simple">
<li>use <code class="docutils literal"><span class="pre">pyspark.sql.functions.lit</span></code>, <strong>literals</strong></li>
</ol>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">lit</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="mf">23.0</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mf">23.0</span><span class="p">)],</span> <span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">,</span> <span class="s2">&quot;x2&quot;</span><span class="p">,</span> <span class="s2">&quot;x3&quot;</span><span class="p">))</span>
<span class="n">df_with_x4</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;x4&quot;</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="n">df_with_x4</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">## +---+---+-----+---+</span>
<span class="c1">## | x1| x2|   x3| x4|</span>
<span class="c1">## +---+---+-----+---+</span>
<span class="c1">## |  1|  a| 23.0|  0|</span>
<span class="c1">## |  3|  B|-23.0|  0|</span>
<span class="c1">## +---+---+-----+---+</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li>transorm an existing column</li>
</ol>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">exp</span>
<span class="n">df_with_x5</span> <span class="o">=</span> <span class="n">df_with_x4</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;x5&quot;</span><span class="p">,</span> <span class="n">exp</span><span class="p">(</span><span class="s2">&quot;x3&quot;</span><span class="p">))</span>
<span class="n">df_with_x5</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">## +---+---+-----+---+--------------------+</span>
<span class="c1">## | x1| x2|   x3| x4|                  x5|</span>
<span class="c1">## +---+---+-----+---+--------------------+</span>
<span class="c1">## |  1|  a| 23.0|  0| 9.744803446248903E9|</span>
<span class="c1">## |  3|  B|-23.0|  0|1.026187963170189...|</span>
<span class="c1">## +---+---+-----+---+--------------------+</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li>use <code class="docutils literal"><span class="pre">.join</span></code></li>
</ol>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">lookup</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;foo&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;bar&quot;</span><span class="p">)],</span> <span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">))</span>
<span class="n">df_with_x6</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_with_x5</span>
    <span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lookup</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">col</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">),</span> <span class="s2">&quot;leftouter&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="s2">&quot;x6&quot;</span><span class="p">))</span>
<span class="c1">## +---+---+-----+---+--------------------+----+</span>
<span class="c1">## | x1| x2|   x3| x4|                  x5|  x6|</span>
<span class="c1">## +---+---+-----+---+--------------------+----+</span>
<span class="c1">## |  1|  a| 23.0|  0| 9.744803446248903E9| foo|</span>
<span class="c1">## |  3|  B|-23.0|  0|1.026187963170189...|null|</span>
<span class="c1">## +---+---+-----+---+--------------------+----+</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li>use udf/function</li>
</ol>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">rand</span>
<span class="n">df_with_x7</span> <span class="o">=</span> <span class="n">df_with_x6</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;x7&quot;</span><span class="p">,</span> <span class="n">rand</span><span class="p">())</span>
<span class="n">df_with_x7</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">## +---+---+-----+---+--------------------+----+-------------------+</span>
<span class="c1">## | x1| x2|   x3| x4|                  x5|  x6|                 x7|</span>
<span class="c1">## +---+---+-----+---+--------------------+----+-------------------+</span>
<span class="c1">## |  1|  a| 23.0|  0| 9.744803446248903E9| foo|0.41930610446846617|</span>
<span class="c1">## |  3|  B|-23.0|  0|1.026187963170189...|null|0.37801881545497873|</span>
<span class="c1">## +---+---+-----+---+--------------------+----+-------------------+</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="handy-snippets">
<h2><a class="toc-backref" href="#id5">5.2. Handy snippets</a><a class="headerlink" href="#handy-snippets" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://spark.apache.org/docs/latest/quick-start.html">https://spark.apache.org/docs/latest/quick-start.html</a></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># this creates an RDD object</span>
<span class="n">textFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;README.md&quot;</span><span class="p">)</span>

<span class="n">textFile</span><span class="o">.</span><span class="n">count</span><span class="p">()</span> <span class="c1"># Number of items in this RDD</span>
<span class="mi">126</span>

<span class="n">textFile</span><span class="o">.</span><span class="n">first</span><span class="p">()</span> <span class="c1"># First item in this RDD</span>
<span class="s1">u&#39;# Apache Spark&#39;</span>

<span class="n">linesWithSpark</span> <span class="o">=</span> <span class="n">textFile</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="s2">&quot;Spark&quot;</span> <span class="ow">in</span> <span class="n">line</span><span class="p">)</span>
<span class="n">textFile</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="s2">&quot;Spark&quot;</span> <span class="ow">in</span> <span class="n">line</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span> <span class="c1"># How many lines contain &quot;Spark&quot;?</span>
<span class="mi">15</span>

<span class="c1"># find the line with the most words</span>
<span class="n">textFile</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()))</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="n">b</span><span class="p">)</span> <span class="k">else</span> <span class="n">b</span><span class="p">)</span>
<span class="mi">15</span>

<span class="c1"># we can also pass a top-level python function (instead of anonymous functions like above)</span>
<span class="k">def</span> <span class="nf">max</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="n">b</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">b</span>
<span class="n">textFile</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()))</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="nb">max</span><span class="p">)</span>

<span class="c1">#======================================================================#</span>
<span class="c1"># One common data flow pattern is MapReduce, as popularized by Hadoop.</span>
<span class="c1"># Spark can implement MapReduce flows easily:</span>
<span class="c1">#======================================================================#</span>
<span class="c1"># compute the per-word counts in the file as an RDD of (string, int) pairs</span>
<span class="n">wordCounts</span> <span class="o">=</span> <span class="p">(</span><span class="n">textFile</span>
                <span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
                <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span><span class="o">+</span><span class="n">b</span><span class="p">))</span>

<span class="n">wordCounts</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="p">[(</span><span class="s1">u&#39;and&#39;</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="p">(</span><span class="s1">u&#39;A&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">u&#39;webpage&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">u&#39;README&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">u&#39;Note&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">u&#39;&quot;local&quot;&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">u&#39;variable&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="o">...</span><span class="p">]</span>

<span class="c1"># caching can help when you query small &quot;hot&quot; dataset or running iterative</span>
<span class="c1"># alg. like page-rank</span>
<span class="n">linesWithSpark</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">linesWithSpark</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="mi">19</span>
<span class="n">linesWithSpark</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="mi">19</span>
</pre></div>
</div>
<div class="section" id="todo-run-this-script">
<h3><a class="toc-backref" href="#id6">5.2.1. Todo: run this script</a><a class="headerlink" href="#todo-run-this-script" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications">https://spark.apache.org/docs/latest/quick-start.html#self-contained-applications</a></p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;SimpleApp.py&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>

<span class="n">logFile</span> <span class="o">=</span> <span class="s2">&quot;YOUR_SPARK_HOME/README.md&quot;</span>  <span class="c1"># Should be some file on your system</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="s2">&quot;local&quot;</span><span class="p">,</span> <span class="s2">&quot;Simple App&quot;</span><span class="p">)</span>
<span class="n">logData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="n">logFile</span><span class="p">)</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>

<span class="n">numAs</span> <span class="o">=</span> <span class="n">logData</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="s1">&#39;a&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
<span class="n">numBs</span> <span class="o">=</span> <span class="n">logData</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="s1">&#39;b&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Lines with a: </span><span class="si">%i</span><span class="s2">, lines with b: </span><span class="si">%i</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">numAs</span><span class="p">,</span> <span class="n">numBs</span><span class="p">))</span>
</pre></div>
</div>
<p>Submit this script using <code class="docutils literal"><span class="pre">bin/spark-submit</span></code> script</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="c1"># Use spark-submit to run your application</span>
$ YOUR_SPARK_HOME/bin/spark-submit --master local<span class="o">[</span>4<span class="o">]</span> SimpleApp.py
...
Lines with a: 46, Lines with b: 23
</pre></div>
</div>
</div>
<div class="section" id="bunch-of-example-scripts">
<h3><a class="toc-backref" href="#id7">5.2.2. Bunch of example scripts</a><a class="headerlink" href="#bunch-of-example-scripts" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://github.com/apache/spark/tree/master/examples/src/main/python">https://github.com/apache/spark/tree/master/examples/src/main/python</a></p>
</div>
<div class="section" id="use-of-persist-rdd-persistence">
<h3><a class="toc-backref" href="#id8">5.2.3. Use of persist (RDD persistence)</a><a class="headerlink" href="#use-of-persist-rdd-persistence" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://spark.apache.org/docs/latest/programming-guide.html#basics">https://spark.apache.org/docs/latest/programming-guide.html#basics</a></li>
<li><a class="reference external" href="https://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence">https://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence</a></li>
</ul>
<p>One of the most important capabilities in Spark is persisting (or caching) a
dataset in memory across operations. When you persist an RDD, each node stores
any partitions of it that it computes in memory and reuses them in other
actions on that dataset (or datasets derived from it). This allows future
actions to be much faster (often by more than 10x). <strong>Caching is a key tool for
iterative algorithms and fast interactive use</strong>.</p>
<p>You can mark an RDD to be persisted using the <code class="docutils literal"><span class="pre">persist()</span></code> or <code class="docutils literal"><span class="pre">cache()</span></code> methods on it.</p>
<ul class="simple">
<li>The first time it is computed in an action, it will be kept in memory on the nodes.</li>
<li>Spark&#8217;s cache is <strong>fault-tolerant</strong> – if any partition of an RDD is lost,
it will automatically be recomputed using the transformations that
originally created it.</li>
</ul>
<p>In addition, each persisted RDD can be stored using a different <strong>storage level</strong>,
allowing you, for example, to persist the dataset on disk, persist it in
memory but as serialized Java objects (to save space), replicate it across nodes.</p>
<ul class="simple">
<li>These levels are set by passing a <code class="docutils literal"><span class="pre">StorageLevel</span></code> object (Scala, Java, Python) to <code class="docutils literal"><span class="pre">persist()</span></code>.</li>
<li>The <code class="docutils literal"><span class="pre">cache()</span></code> method is a shorthand for using the default storage level,
which is <code class="docutils literal"><span class="pre">StorageLevel.MEMORY_ONLY</span></code> (store deserialized objects in memory).</li>
<li>The full set of storage levels is
(<a class="reference external" href="https://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence">link</a>)</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span class="p">)</span>
<span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># if you want to use lineLengths again later, do this before reduce</span>
<span class="c1"># (saves the data in memory)</span>
<span class="n">lineLengths</span><span class="o">.</span><span class="n">persist</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="sql-in-pyspark">
<h2><a class="toc-backref" href="#id9">5.3. SQL in pyspark</a><a class="headerlink" href="#sql-in-pyspark" title="Permalink to this headline">¶</a></h2>
<p>Using Spark SQL within a Python Notebook</p>
<p>You can use execute SQL commands within a python notebook by invoking %sql or using <code class="docutils literal"><span class="pre">sqlContext.sql(...)</span></code>.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">sql</span> <span class="n">show</span> <span class="n">functions</span>
</pre></div>
</div>
</div>
<div class="section" id="spark-sql-dataframes-and-datasets-guide">
<h2><a class="toc-backref" href="#id10">5.4. Spark SQL, DataFrames and Datasets Guide</a><a class="headerlink" href="#spark-sql-dataframes-and-datasets-guide" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://spark.apache.org/docs/latest/sql-programming-guide.html">https://spark.apache.org/docs/latest/sql-programming-guide.html</a></p>
<div class="section" id="dataframes">
<h3><a class="toc-backref" href="#id11">5.4.1. DataFrames</a><a class="headerlink" href="#dataframes" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1">#======================================================================#</span>
<span class="c1"># create a basic SparkSession using SparkSession.builder</span>
<span class="c1">#======================================================================#</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="c1"># SparkSession in Spark 2.0 provides builtin support for Hive features</span>
<span class="c1"># including the ability to write queries using HiveQL</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span>\
    <span class="o">.</span><span class="n">builder</span>\
    <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;PythonSQL&quot;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.some.config.option&quot;</span><span class="p">,</span> <span class="s2">&quot;some-value&quot;</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="c1"># spark is an existing SparkSession</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">)</span>

<span class="c1"># Displays the content of the DataFrame to stdout</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">## age  name</span>
<span class="c1">## null Michael</span>
<span class="c1">## 30   Andy</span>
<span class="c1">## 19   Justin</span>

<span class="c1"># Print the schema in a tree format</span>
<span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="c1">## root</span>
<span class="c1">## |-- age: long (nullable = true)</span>
<span class="c1">## |-- name: string (nullable = true)</span>

<span class="c1"># Select only the &quot;name&quot; column</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Select everybody, but increment the age by 1</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">## name    (age + 1)</span>
<span class="c1">## Michael null</span>
<span class="c1">## Andy    31</span>
<span class="c1">## Justin  20</span>

<span class="c1"># Select people older than 21</span>
<span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">21</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="c1">## age name</span>
<span class="c1">## 30  Andy</span>

<span class="c1">#======================================================================#</span>
<span class="c1"># run SQL Queries programatically</span>
<span class="c1">#======================================================================#</span>
<span class="c1"># spark is an existing SparkSession</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM table&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="inter-operating-rdd-dataframes">
<h3><a class="toc-backref" href="#id12">5.4.2. Inter-operating RDD &amp; DataFrames</a><a class="headerlink" href="#inter-operating-rdd-dataframes" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal"><span class="pre">Rows</span></code> are constructed from a list of key/value pairs. The key will be
inferred as the column names of the table.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># spark is an existing SparkSession.</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span>

<span class="c1"># Load a text file and convert each line to a Row.</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.txt&quot;</span><span class="p">)</span>
<span class="n">parts</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))</span>
<span class="n">people</span> <span class="o">=</span> <span class="n">parts</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">age</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>

<span class="c1">#======================================================================#</span>
<span class="c1"># Here, create DF from Row object by inferring scheme</span>
<span class="c1"># (key values will be used as column names)</span>
<span class="c1">#======================================================================#</span>
<span class="c1"># Infer the schema, and register the DataFrame as a table.</span>
<span class="n">schemaPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">people</span><span class="p">)</span>
<span class="n">schemaPeople</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># SQL can be run over DataFrames that have been registered as a table.</span>
<span class="n">teenagers</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>

<span class="c1"># The results of SQL queries are RDDs and support all the normal RDD operations.</span>
<span class="n">teenNames</span> <span class="o">=</span> <span class="n">teenagers</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="s2">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="k">for</span> <span class="n">teenName</span> <span class="ow">in</span> <span class="n">teenNames</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="n">teenName</span><span class="p">)</span>

<span class="c1">#======================================================================#</span>
<span class="c1"># Programmatically Specifying the Schema</span>
<span class="c1">#======================================================================#</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># The schema is encoded in a string.</span>
<span class="n">schemaString</span> <span class="o">=</span> <span class="s2">&quot;name age&quot;</span>

<span class="n">fields</span> <span class="o">=</span> <span class="p">[</span><span class="n">StructField</span><span class="p">(</span><span class="n">field_name</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="bp">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">field_name</span> <span class="ow">in</span> <span class="n">schemaString</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
<span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">(</span><span class="n">fields</span><span class="p">)</span>

<span class="c1"># Apply the schema to the RDD.</span>
<span class="n">schemaPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">people</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>

<span class="c1"># Creates a temporary view using the DataFrame</span>
<span class="n">schemaPeople</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># SQL can be run over DataFrames that have been registered as a table.</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM people&quot;</span><span class="p">)</span>

<span class="c1"># The results of SQL queries are RDDs and support all the normal RDD operations.</span>
<span class="n">names</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="s2">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="data-sources">
<h3><a class="toc-backref" href="#id13">5.4.3. Data Sources</a><a class="headerlink" href="#data-sources" title="Permalink to this headline">¶</a></h3>
<p><strong>Registering</strong> a DataFrame as a <strong>temporary view</strong> allows you to run SQL queries over its data</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># simplest load/save (default datasource = parquet)</span>
<span class="c1"># (default can be configureated in spark.sql.sources.default)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/users.parquet&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;favorite_color&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;namesAndFavColors.parquet&quot;</span><span class="p">)</span>

<span class="c1"># or you can manually specify options</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s2">&quot;json&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;namesAndAges.parquet&quot;</span><span class="p">,</span> <span class="n">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">)</span>

<span class="c1"># can run sql queries on files directly</span>
<span class="c1"># (Instead of using read API to load a file into DataFrame and query it,</span>
<span class="c1">#  you can also query that file directly with SQL.)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;</span><span class="p">)</span>

<span class="c1"># for save modes, see:</span>
<span class="c1"># https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</span>
</pre></div>
</div>
</div>
<div class="section" id="saving-to-persistent-tables">
<h3><a class="toc-backref" href="#id14">5.4.4. Saving to Persistent Tables</a><a class="headerlink" href="#saving-to-persistent-tables" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes">https://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a></p>
<p>DataFrames can also be saved as persistent tables into Hive metastore using
the saveAsTable command. Notice existing Hive deployment is not necessary to
use this feature. Spark will create a default local Hive metastore (using
Derby) for you. Unlike the createOrReplaceTempView command, saveAsTable will
materialize the contents of the DataFrame and create a pointer to the data in
the Hive metastore. Persistent tables will still exist even after your Spark
program has restarted, as long as you maintain your connection to the same
metastore. A DataFrame for a persistent table can be created by calling the
table method on a SparkSession with the name of the table.</p>
<p>By default saveAsTable will create a “managed table”, meaning that the
location of the data will be controlled by the metastore. Managed tables will
also have their data deleted automatically when a table is dropped.</p>
<div class="section" id="parquet-files">
<h4>5.4.4.1. Parquet Files<a class="headerlink" href="#parquet-files" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="http://parquet.io/">Parquet</a> is a columnar format that is supported by many
other data processing systems.</p>
<ul class="simple">
<li>Spark SQL provides support for both reading and writing Parquet files that
automatically preserves the schema of the original data.</li>
<li>When writing Parquet files, all columns are automatically converted to be
nullable for compatibility reasons.</li>
<li>The loaded parquet files are DataFrames</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># spark from the previous example is used in this example.</span>

<span class="n">schemaPeople</span> <span class="c1"># The DataFrame from the previous example.</span>

<span class="c1"># DataFrames can be saved as Parquet files, maintaining the schema information.</span>
<span class="n">schemaPeople</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;people.parquet&quot;</span><span class="p">)</span>

<span class="c1"># Read in the Parquet file created above. Parquet files are self-describing so the schema is preserved.</span>
<span class="c1"># The result of loading a parquet file is also a DataFrame.</span>
<span class="n">parquetFile</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;people.parquet&quot;</span><span class="p">)</span>

<span class="c1"># Parquet files can also be used to create a temporary view and then used in SQL statements.</span>
<span class="n">parquetFile</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;parquetFile&quot;</span><span class="p">);</span>
<span class="n">teenagers</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>
<span class="n">teenNames</span> <span class="o">=</span> <span class="n">teenagers</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="s2">&quot;Name: &quot;</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
<span class="k">for</span> <span class="n">teenName</span> <span class="ow">in</span> <span class="n">teenNames</span><span class="o">.</span><span class="n">collect</span><span class="p">():</span>
  <span class="k">print</span><span class="p">(</span><span class="n">teenName</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="schema-merging">
<h4>5.4.4.2. Schema merging<a class="headerlink" href="#schema-merging" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://spark.apache.org/docs/latest/sql-programming-guide.html#schema-merging">https://spark.apache.org/docs/latest/sql-programming-guide.html#schema-merging</a></p>
<blockquote>
<div>Not a necessity in most cases.</div></blockquote>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># spark from the previous example is used in this example.</span>

<span class="c1"># Create a simple DataFrame, stored into a partition directory</span>
<span class="n">df1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>\
                                   <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">single</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">double</span><span class="o">=</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">df1</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/test_table/key=1&quot;</span><span class="p">)</span>

<span class="c1"># Create another DataFrame in a new partition directory,</span>
<span class="c1"># adding a new column and dropping an existing column</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>
                                   <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">Row</span><span class="p">(</span><span class="n">single</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">triple</span><span class="o">=</span><span class="n">i</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)))</span>
<span class="n">df2</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/test_table/key=2&quot;</span><span class="p">)</span>

<span class="c1"># Read the partitioned table</span>
<span class="n">df3</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">option</span><span class="p">(</span><span class="s2">&quot;mergeSchema&quot;</span><span class="p">,</span> <span class="s2">&quot;true&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="s2">&quot;data/test_table&quot;</span><span class="p">)</span>
<span class="n">df3</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>

<span class="c1"># The final schema consists of all 3 columns in the Parquet files together</span>
<span class="c1"># with the partitioning column appeared in the partition directory paths.</span>
<span class="c1"># root</span>
<span class="c1"># |-- single: int (nullable = true)</span>
<span class="c1"># |-- double: int (nullable = true)</span>
<span class="c1"># |-- triple: int (nullable = true)</span>
<span class="c1"># |-- key : int (nullable = true)</span>
</pre></div>
</div>
</div>
<div class="section" id="json-datasets">
<h4>5.4.4.3. JSON Datasets<a class="headerlink" href="#json-datasets" title="Permalink to this headline">¶</a></h4>
<p>Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame</p>
<ul class="simple">
<li>Note that the file that is offered as a json file is not a typical JSON file.</li>
<li>Each line must contain a separate, self-contained valid JSON object.</li>
<li>As a consequence, a regular multi-line JSON file will most often fail.</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># spark is an existing SparkSession.</span>

<span class="c1"># A JSON dataset is pointed to by path.</span>
<span class="c1"># The path can be either a single text file or a directory storing text files.</span>
<span class="n">people</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="s2">&quot;examples/src/main/resources/people.json&quot;</span><span class="p">)</span>

<span class="c1"># The inferred schema can be visualized using the printSchema() method.</span>
<span class="n">people</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="c1"># root</span>
<span class="c1">#  |-- age: long (nullable = true)</span>
<span class="c1">#  |-- name: string (nullable = true)</span>

<span class="c1"># Creates a temporary view using the DataFrame.</span>
<span class="n">people</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s2">&quot;people&quot;</span><span class="p">)</span>

<span class="c1"># SQL statements can be run by using the sql methods provided by `spark`.</span>
<span class="n">teenagers</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;</span><span class="p">)</span>

<span class="c1"># Alternatively, a DataFrame can be created for a JSON dataset represented by</span>
<span class="c1"># an RDD[String] storing one JSON object per string.</span>
<span class="n">anotherPeopleRDD</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span>
  <span class="s1">&#39;{&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:{&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;}}&#39;</span><span class="p">])</span>
<span class="n">anotherPeople</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">jsonRDD</span><span class="p">(</span><span class="n">anotherPeopleRDD</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="programming-guide-condensed-summary">
<h2><a class="toc-backref" href="#id15">5.5. Programming-guide: condensed summary</a><a class="headerlink" href="#programming-guide-condensed-summary" title="Permalink to this headline">¶</a></h2>
<p>From <a class="reference external" href="http://spark.apache.org/docs/latest/programming-guide.html">http://spark.apache.org/docs/latest/programming-guide.html</a></p>
<div class="section" id="super-basics">
<h3><a class="toc-backref" href="#id16">5.5.1. Super basics</a><a class="headerlink" href="#super-basics" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>use the <code class="docutils literal"><span class="pre">bin/spark-submit</span></code> script in the Spark directory to run Spark applications in Python</li>
</ul>
<div class="highlight-bash"><div class="highlight"><pre><span></span>$ <span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>python3.4 bin/pyspark
$ <span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>/opt/pypy-2.5/bin/pypy bin/spark-submit examples/src/main/python/pi.py
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">SparkConf</span>

<span class="c1">#=========================================================================#</span>
<span class="c1"># 1st thing a Spark program must do: create SC object that tells Spark how to access a cluster</span>
<span class="c1">#=========================================================================#</span>
<span class="c1"># create config object (contains information about your application)</span>
<span class="c1"># - `appName` = name of the application to show on the cluster UI</span>
<span class="c1"># - `master` = &quot;local&quot;, or URL to Spark, Mesos, or YARN cluster.</span>
<span class="c1">#   (http://spark.apache.org/docs/latest/submitting-applications.html#master-urls)</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="n">appName</span><span class="p">)</span><span class="o">.</span><span class="n">setMaster</span><span class="p">(</span><span class="n">master</span><span class="p">)</span>

<span class="c1"># create SparkContext object</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>

<span class="c1"># === create RDD from an existing collection/iterable ===</span>
<span class="c1"># - use sc.parallelize</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">distData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="c1"># create Parallelized collections</span>
<span class="n">distData</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># partitions (typically 2-4 partitions for each CPU in cluster</span>
<span class="n">distData</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">partitions</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># can also specify</span>
<span class="n">wordsRDD</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="s2">&quot;fish&quot;</span><span class="p">,</span> <span class="s2">&quot;cats&quot;</span><span class="p">,</span> <span class="s2">&quot;dogs&quot;</span><span class="p">])</span>


<span class="c1"># === RDD external dataset ===</span>
<span class="c1"># - use sc.textFile</span>
<span class="c1"># URI = either a local path on the machine, or a hdfs://, s3n://, etc URI</span>
<span class="n">distFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span class="p">)</span>
<span class="n">distFile</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># All of Spark’s file-based input methods, including textFile,</span>
<span class="c1"># support running on directories, compressed files, and wildcards</span>
<span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;/my/directory&quot;</span><span class="p">)</span>
<span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;/my/directory/*.txt&quot;</span><span class="p">)</span>
<span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;/my/directory/*.gz&quot;</span><span class="p">)</span>


<span class="c1">#==========================================================================#</span>
<span class="c1"># saving and loading</span>
<span class="c1">#==========================================================================#</span>
<span class="c1"># Similarly to text files, SequenceFiles can be saved and loaded by specifying the path</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span> <span class="o">*</span> <span class="n">x</span> <span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">saveAsSequenceFile</span><span class="p">(</span><span class="s2">&quot;path/to/file&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">sequenceFile</span><span class="p">(</span><span class="s2">&quot;path/to/file&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>
<span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">u&#39;a&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s1">u&#39;aa&#39;</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s1">u&#39;aaa&#39;</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="section" id="shared-variables">
<h3><a class="toc-backref" href="#id17">5.5.2. Shared variables</a><a class="headerlink" href="#shared-variables" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>General, read-write shared variables across tasks would be inefficient.</li>
<li>However, Spark does provide two limited types of shared variables for two common usage patterns: broadcast variables and accumulators.</li>
</ul>
<div class="section" id="broadast-variables">
<h4>5.5.2.1. Broadast variables<a class="headerlink" href="#broadast-variables" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">SparkContext.broadcast(v)</span></code> - creates Broadcast variables from variable v.<ul>
<li>The broadcast variable is a wrapper around v, and its value can be accessed by
calling the <code class="docutils literal"><span class="pre">.value</span></code> method</li>
</ul>
</li>
<li>Broadcast variables are used to keep a read-only variable cached on each machine
(rather than shipping a copy of it with tasks).<ul>
<li>example usage: to give every node a copy of a large input dataset in an efficient manner.</li>
</ul>
</li>
<li>explicitly creating broadcast variables is only useful when tasks across multiple
stages need the same data or when caching the data in deserialized form is important.</li>
</ul>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">broadcastVar</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="go">&lt;pyspark.broadcast.Broadcast object at 0x102789f10&gt;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">broadcastVar</span><span class="o">.</span><span class="n">value</span>
<span class="go">[1, 2, 3]</span>
</pre></div>
</div>
</div>
<div class="section" id="accumulators">
<h4>5.5.2.2. Accumulators<a class="headerlink" href="#accumulators" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference external" href="http://spark.apache.org/docs/latest/programming-guide.html#accumulators">http://spark.apache.org/docs/latest/programming-guide.html#accumulators</a></p>
</div>
</div>
<div class="section" id="rdd-operations">
<h3><a class="toc-backref" href="#id18">5.5.3. RDD operations</a><a class="headerlink" href="#rdd-operations" title="Permalink to this headline">¶</a></h3>
<div class="section" id="rdd-basics">
<h4>5.5.3.1. RDD-basics<a class="headerlink" href="#rdd-basics" title="Permalink to this headline">¶</a></h4>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span class="p">)</span>
<span class="n">lineLengths</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="n">totalLength</span> <span class="o">=</span> <span class="n">lineLengths</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="n">lineLengths</span><span class="o">.</span><span class="n">persist</span><span class="p">()</span> <span class="c1"># if you want to use this object again later</span>
</pre></div>
</div>
</div>
<div class="section" id="passing-functions-to-spark">
<h4>5.5.3.2. Passing functions to Spark<a class="headerlink" href="#passing-functions-to-spark" title="Permalink to this headline">¶</a></h4>
<p>Spark relies heavily on passing functions in the driver program to run on the cluster.</p>
<p>3 recommended ways to do this:</p>
<p>1. lambda expressions for simple functions (does not support mult-statement functions or statements that do not return a value)
2. Local <code class="docutils literal"><span class="pre">def</span></code> functions
3. Top-level functions in a module</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">myFunc</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;file.txt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">myFunc</span><span class="p">)</span>
</pre></div>
</div>
<p>Some caveats when defining class attributes</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># don&#39;t do this (the whole object gets sent to the luster when ``doStuff`` is called)</span>
<span class="k">class</span> <span class="nc">MyClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">s</span>
    <span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">)</span>

<span class="c1"># or this (accessing fields of the outer object will reference the ENTIRE object)</span>
<span class="k">class</span> <span class="nc">MyClass</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">field</span> <span class="o">=</span> <span class="s2">&quot;Hello&quot;</span>
    <span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">field</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span>

<span class="c1"># rather, do this (copy field into a local variable instead of accessing it externally)</span>
<span class="k">def</span> <span class="nf">doStuff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">):</span>
    <span class="n">field</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">field</span>
    <span class="k">return</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">field</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="rdd-transformations">
<h4>5.5.3.3. RDD Transformations<a class="headerlink" href="#rdd-transformations" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="http://spark.apache.org/docs/latest/programming-guide.html#transformations">http://spark.apache.org/docs/latest/programming-guide.html#transformations</a></p>
</div>
<div class="section" id="rdd-actions">
<h4>5.5.3.4. RDD Actions<a class="headerlink" href="#rdd-actions" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="http://spark.apache.org/docs/latest/programming-guide.html#actions">http://spark.apache.org/docs/latest/programming-guide.html#actions</a></p>
</div>
<div class="section" id="closures">
<h4>5.5.3.5. closures<a class="headerlink" href="#closures" title="Permalink to this headline">¶</a></h4>
<p>Common confusion in Spark:</p>
<ul class="simple">
<li>understanding the <strong>scope</strong> and <strong>life cycle</strong> of variables and methods when
executing code across a cluster.</li>
<li>In general, <strong>closures</strong> - constructs like loops or locally defined methods,
should not be used to mutate some global state.</li>
<li>Use an <strong>Accumulator</strong> instead if some global aggregation is needed.</li>
</ul>
<p>Example: wrong way to increment a counter</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Wrong: Don&#39;t do this!! (will only work in master=&quot;local&quot; mode, but won&#39;t work on cluster)</span>
<span class="k">def</span> <span class="nf">increment_counter</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">counter</span>
    <span class="n">counter</span> <span class="o">+=</span> <span class="n">x</span>
<span class="n">rdd</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">increment_counter</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Counter value: &quot;</span><span class="p">,</span> <span class="n">counter</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="working-with-key-value-pairs">
<h4>5.5.3.6. Working with key-value pairs<a class="headerlink" href="#working-with-key-value-pairs" title="Permalink to this headline">¶</a></h4>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span class="p">)</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">pairs</span><span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="n">counts</span><span class="o">.</span><span class="n">sortByKey</span><span class="p">()</span> <span class="c1"># sort alphabetically</span>
<span class="n">counts</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span> <span class="c1"># bring them back to the driver program as a list of objects</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="random-handy-snippets">
<h2><a class="toc-backref" href="#id19">5.6. Random handy snippets</a><a class="headerlink" href="#random-handy-snippets" title="Permalink to this headline">¶</a></h2>
<div class="section" id="compute-average">
<h3><a class="toc-backref" href="#id20">5.6.1. compute average</a><a class="headerlink" href="#compute-average" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="c1"># using RDDs</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">_</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span>
<span class="n">rdd</span><span class="o">.</span><span class="n">map</span> <span class="p">{</span> <span class="n">x</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="n">x</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">x</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">toFloat</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="p">}</span><span class="o">.</span>
<span class="n">reduceByKey</span> <span class="p">{</span> <span class="n">case</span> <span class="p">((</span><span class="n">num1</span><span class="p">,</span> <span class="n">count1</span><span class="p">),</span> <span class="p">(</span><span class="n">num2</span><span class="p">,</span> <span class="n">count2</span><span class="p">))</span> <span class="o">=&gt;</span>
<span class="p">(</span><span class="n">num1</span> <span class="o">+</span> <span class="n">num2</span><span class="p">,</span> <span class="n">count1</span> <span class="o">+</span> <span class="n">count2</span><span class="p">)</span>
<span class="p">}</span><span class="o">.</span>
<span class="nb">map</span> <span class="p">{</span> <span class="n">case</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">count</span><span class="p">))</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">num</span> <span class="o">/</span> <span class="n">count</span><span class="p">)</span> <span class="p">}</span><span class="o">.</span>
<span class="n">collect</span><span class="p">()</span>

<span class="c1"># using DF</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql.functions._</span>
<span class="n">val</span> <span class="n">df</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">a</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="n">a</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">a</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;key&quot;</span><span class="p">)</span>
<span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">avg</span><span class="p">(</span><span class="s2">&quot;value&quot;</span><span class="p">))</span>
<span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="bct.html" class="btn btn-neutral float-right" title="6. Brain Connectivity Toolbox" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cs-python.html" class="btn btn-neutral" title="4. Python" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, tw.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>